# Profiler API

Psychological profiling service using LLM analysis

## ğŸŒ Production Environment

**URL**: `https://api.hey-watch.me/profiler/`

### Current Status

**âœ… Production Ready** (2025-11-13)

| Item | Value |
|------|-------|
| **LLM Provider** | Groq |
| **Model** | openai/gpt-oss-120b (reasoning model) |
| **Reasoning Effort** | medium |
| **Deploy Date** | 2025-11-13 |
| **Status** | healthy âœ… |

**Health Check**:
```bash
curl https://api.hey-watch.me/profiler/health | jq
```

**Expected Response**:
```json
{
  "status": "healthy",
  "llm_provider": "groq",
  "llm_model": "openai/gpt-oss-120b"
}
```

### Infrastructure

- Microservice architecture
- SSL/HTTPS enabled, CORS configured
- Docker + ECR + systemd
- CI/CD auto-deploy (GitHub Actions)

---

## ğŸ¯ Overview

This API provides psychological profiling by analyzing aggregated audio data using LLM (ChatGPT/Groq).

### Main Features

- **Spot Profiler**: Single recording analysis (`/spot-profiler`)
- **Daily Profiler**: Daily cumulative analysis (`/daily-profiler`) ğŸš§ Coming soon
- **Weekly Profiler**: Weekly trend analysis (`/weekly-profiler`) ğŸš§ Coming soon
- **Monthly Profiler**: Monthly long-term analysis (`/monthly-profiler`) ğŸš§ Coming soon
- **Multiple LLM Provider Support**: Easy switching between OpenAI, Groq, etc.
- **Retry Functionality**: Ensures API call stability

---

## ğŸ—ºï¸ Routing Details

| Item | Value | Description |
|------|-------|-------------|
| **ğŸ·ï¸ Service Name** | Profiler API | LLM psychological profiling |
| **ğŸ“¦ Function** | LLM Gateway | ChatGPT/Groq analysis execution |
| | | |
| **ğŸŒ External Access (Nginx)** | | |
| â”” Public Endpoint | `https://api.hey-watch.me/profiler/` | âœ… Unified naming convention |
| â”” Nginx Config File | `/etc/nginx/sites-available/api.hey-watch.me` | |
| â”” proxy_pass Target | `http://localhost:8051/` | Internal forwarding |
| â”” Timeout | 180 seconds | read/connect/send |
| | | |
| **ğŸ”Œ API Internal Endpoints** | | |
| â”” Health Check | `/health` | GET |
| â”” **Spot Profiler** | `/spot-profiler` | POST - Called by Lambda |
| â”” **Daily Profiler** | `/daily-profiler` | POST - Daily summary (ğŸš§ Coming soon) |
| â”” **Weekly Profiler** | `/weekly-profiler` | POST - Weekly analysis (ğŸš§ Coming soon) |
| â”” **Monthly Profiler** | `/monthly-profiler` | POST - Monthly analysis (ğŸš§ Coming soon) |
| | | |
| **ğŸ³ Docker/Container** | | |
| â”” Container Name | `profiler-api` | âœ… Unified naming |
| â”” Port (Internal) | 8051 | Inside container |
| â”” Port (Public) | `127.0.0.1:8051:8051` | localhost only |
| â”” Health Check | `/health` | Docker healthcheck |
| | | |
| **â˜ï¸ AWS ECR** | | |
| â”” Repository Name | `watchme-profiler-api` | âœ… Unified naming |
| â”” Region | ap-southeast-2 (Sydney) | |
| â”” URI | `754724220380.dkr.ecr.ap-southeast-2.amazonaws.com/watchme-profiler-api:latest` | |
| | | |
| **âš™ï¸ systemd** | | |
| â”” Service Name | `profiler-api.service` | docker-compose management |
| â”” Startup Command | `docker-compose up -d` | |
| â”” Auto Start | enabled | Auto-start on server reboot |
| | | |
| **ğŸ“‚ Directory** | | |
| â”” Source Code | `/Users/kaya.matsumoto/projects/watchme/api/profiler` | Local |
| â”” GitHub Repository | `hey-watchme/api-profiler` | |
| â”” EC2 Location | `/home/ubuntu/profiler-api` | Production execution directory |
| | | |
| **ğŸ”— Caller** | | |
| â”” Lambda Function (Spot) | `watchme-audio-worker` | |
| â”” Call URL (Spot) | âœ… `https://api.hey-watch.me/profiler/spot-profiler` | |
| â”” Environment Variable | `API_BASE_URL=https://api.hey-watch.me` | Inside Lambda |

---

## ğŸ¤– LLM Provider Settings

### Design Concept

This API **supports multiple LLM providers** and can be easily switched.

**Purpose**:
- Quick migration to new models
- Cost optimization (different providers have different pricing)
- Immediate rollback on performance issues
- Multiple models prepared in advance (API keys configured, can switch anytime)

**Features**:
- âœ… No client-side changes required (app/other APIs)
- âœ… 1-line code change â†’ git push to switch
- âœ… Can keep 3-5 providers on standby
- âœ… Model version upgrades follow same procedure

### Currently In Use

- Provider: **Groq**
- Model: **openai/gpt-oss-120b** (reasoning model)
- Reasoning Effort: **medium**

### Supported Providers

| Provider | Example Models | Environment Variable | Status |
|----------|---------------|---------------------|--------|
| **OpenAI** | gpt-4o, gpt-4o-mini, gpt-5-nano, o1-preview | OPENAI_API_KEY | âœ… Configured |
| **Groq** | llama-3.3-70b-versatile, llama-3.1-8b-instant | GROQ_API_KEY | âœ… Configured |
| **Groq via OpenAI** | openai/gpt-oss-120b (reasoning model) | GROQ_API_KEY | âœ… Configured (currently in use) |

### How to Switch Providers

See `llm_providers.py` - change `CURRENT_PROVIDER` and `CURRENT_MODEL` constants.

---

## ğŸ“Œ API Endpoints

### Active Endpoints

| Endpoint | Method | Description |
|----------|---------|-------------|
| `/health` | GET | Health check |
| `/spot-profiler` | POST | Spot profiler analysis (single recording) |
| `/daily-profiler` | POST | Daily profiler analysis (1 day) ğŸš§ Coming soon |
| `/weekly-profiler` | POST | Weekly profiler analysis (7 days) ğŸš§ Coming soon |
| `/monthly-profiler` | POST | Monthly profiler analysis (30 days) ğŸš§ Coming soon |

---

## ğŸ”Œ Endpoint Details

### 1. Health Check

```bash
curl https://api.hey-watch.me/profiler/health
```

**Response:**
```json
{
  "status": "healthy",
  "timestamp": "2025-11-13T00:00:00.000000",
  "llm_provider": "groq",
  "llm_model": "openai/gpt-oss-120b"
}
```

### 2. Spot Profiler

**v1.0.0 Specification**:
- âœ… `recorded_at` parameter (UTC timestamp)
- âœ… Microservice architecture compliant

```bash
curl -X POST https://api.hey-watch.me/profiler/spot-profiler \
  -H "Content-Type: application/json" \
  -d '{
    "device_id": "9f7d6e27-98c3-4c19-bdfb-f7fda58b9a93",
    "recorded_at": "2025-11-13T12:31:01+00:00"
  }'
```

**Processing Flow**:
1. Fetch prompt from `spot_aggregators.prompt`
2. Execute LLM (Groq/ChatGPT) analysis
3. Save result to `spot_results` table

**Response:**
```json
{
  "status": "success",
  "message": "Spot profiler analysis completed (DB save successful)",
  "device_id": "9f7d6e27-98c3-4c19-bdfb-f7fda58b9a93",
  "recorded_at": "2025-11-13T12:31:01+00:00",
  "analysis_result": {
    "summary": "Description of the situation",
    "vibe_score": -30,
    "behavior": "Working"
  },
  "database_save": true,
  "processed_at": "2025-11-13T12:35:00.000Z",
  "model_used": "groq/openai/gpt-oss-120b"
}
```

---

## ğŸ“Š Database Structure

### spot_aggregators Table (Input Source)

**Prompt fetch source**

This API reads prompts from `spot_aggregators.prompt`.

```sql
CREATE TABLE spot_aggregators (
  device_id TEXT NOT NULL,
  recorded_at TIMESTAMPTZ NOT NULL,  -- UTC
  prompt TEXT NOT NULL,
  context_data JSONB,
  created_at TIMESTAMPTZ DEFAULT NOW(),
  PRIMARY KEY (device_id, recorded_at)
);
```

### spot_results Table (Output Destination)

**Spot profiler analysis results**

```sql
CREATE TABLE spot_results (
  device_id TEXT NOT NULL,
  recorded_at TIMESTAMPTZ NOT NULL,  -- UTC
  vibe_score INTEGER CHECK (vibe_score >= -100 AND vibe_score <= 100),
  vibe_summary TEXT,
  vibe_behavior TEXT,
  vibe_scorer_result JSONB,
  vibe_analyzed_at TIMESTAMPTZ,
  created_at TIMESTAMPTZ DEFAULT NOW(),
  PRIMARY KEY (device_id, recorded_at)
);
```

---

## ğŸš€ Deployment

### CI/CD Auto-Deploy

```bash
# Commit & push triggers auto-deploy
git add .
git commit -m "feat: Add new feature"
git push origin main

# GitHub Actions auto-executes (approx. 5 minutes)
# https://github.com/hey-watchme/api-profiler/actions
```

**Auto-execution content:**
1. Build ARM64-compatible Docker image
2. Push to ECR
3. Auto-deploy on EC2
4. Health check

### Service Management Commands (EC2)

```bash
# Check service status
sudo systemctl status profiler-api

# Restart service
sudo systemctl restart profiler-api

# View logs
sudo journalctl -u profiler-api -f
docker logs profiler-api --tail 50
```

---

## ğŸ”§ Environment Variables (.env)

```bash
# LLM API Keys
OPENAI_API_KEY=sk-...
GROQ_API_KEY=gsk-...  # Only when using Groq

# Supabase Settings
SUPABASE_URL=https://qvtlwotzuzbavrzqhyvt.supabase.co
SUPABASE_KEY=your-supabase-key
```

**Note**: Model specification is done in `llm_providers.py` (not environment variables).

---

## ğŸ“¦ Dependencies

```txt
fastapi==0.100.0
uvicorn==0.23.0
pydantic==2.0.2
python-dotenv==1.0.0
openai>=1.0.0
groq>=0.4.0
requests>=2.31.0
python-multipart>=0.0.6
aiohttp>=3.8.0
tenacity>=8.2.0
httpx==0.24.1
gotrue==1.3.0
supabase==2.3.4
```

---

## ğŸ”— Related Services

- **Aggregator API**: `api/aggregator`
- **iOS App**: `ios_watchme_v9`

---

## ğŸ“š API Documentation

- **Swagger UI**: https://api.hey-watch.me/profiler/docs
- **ReDoc**: https://api.hey-watch.me/profiler/redoc

---

## ğŸ“ Changelog

### v1.0.0 (2025-11-13)
- **Breaking Change**: New Profiler API created (separated from Scorer)
- **Breaking Change**: Input table changed to `spot_aggregators.prompt`
- **Breaking Change**: Output table changed to `spot_results`
- Request parameter changed: `date` + `time_block` â†’ `recorded_at` (UTC timestamp)
- Microservice architecture compliance: API reads data from DB itself
- Endpoint name: `/analyze-timeblock` â†’ `/spot-profiler`
- Unified naming convention:
  - Column naming alignment with new architecture
- UTC-unified architecture migration

---

**Developer**: WatchMe
**Version**: 1.0.0
